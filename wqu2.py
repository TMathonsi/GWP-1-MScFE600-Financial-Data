# -*- coding: utf-8 -*-
"""WQU2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10C6AimVMgeO22LFuoF9pzHXShzpLv8Oq
"""

# import essential libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import yfinance as yf
import missingno as msno
from datetime import date, timedelta, datetime
import requests
from io import StringIO
from scipy.interpolate import CubicSpline
from sklearn.decomposition import PCA
from sklearn.impute import KNNImputer
from numpy.linalg import eigh, svd

"""### 2c. Nelson–Siegel fit with MANUAL data entry"""

# Manually create the dataset (from your table)
df_1 = pd.DataFrame({
    "Maturity": ["3M (T-bill)", "5Y", "10Y", "20Y", "30Y"],
    "Tenor_Years": [0.25, 5, 10, 20, 30],
    "Yield_%": [6.55, 7.89, 9.17, 10.32, 10.16],
})
df_1 ["Yield_Decimal"] = df_1 ["Yield_%"] / 100.0

# save the manual dataset for later:
df_1 .to_csv("sa_yield_curve_clean_manual.csv", index=False)

# Quick checks
print(df_1)
print(df_1.info())

def ns_loadings(tau, lam):
    x = tau / lam
    w1 = np.where(np.isclose(x, 0.0), 1.0, (1.0 - np.exp(-x))/x)
    w2 = w1 - np.exp(-x)
    return w1, w2

def fit_given_lambda(tau, ydec, lam):
    w1, w2 = ns_loadings(tau, lam)
    X = np.column_stack([np.ones_like(tau), w1, w2])  # [1, w1, w2]
    beta, *_ = np.linalg.lstsq(X, ydec, rcond=None)
    yhat = X @ beta
    sse = np.sum((ydec - yhat)**2)
    return beta, yhat, sse

def ns_curve(tau, b0, b1, b2, lam):
    w1, w2 = ns_loadings(tau, lam)
    return b0 + b1*w1 + b2*w2

# Fit NS (grid-search lambda, OLS for betas)
m = df_1 ["Tenor_Years"].to_numpy(float)
y = df_1 ["Yield_Decimal"].to_numpy(float)

best = {"lam": None, "beta": None, "yhat": None, "sse": np.inf}
for lam in np.linspace(0.10, 10.0, 2000):
    beta, yhat, sse = fit_given_lambda(m, y, lam)
    if sse < best["sse"]:
        best = {"lam": lam, "beta": beta, "yhat": yhat, "sse": sse}

beta0, beta1, beta2 = best["beta"]
lam = best["lam"]
res = y - best["yhat"]
rmse_bp = 1e4*np.sqrt(np.mean(res**2))
mae_bp  = 1e4*np.mean(np.abs(res))

# Report + Plot
print("Nelson–Siegel parameters")
print(f"  beta0 (level)     = {beta0:.6f}  (~{beta0*100:.2f}%)")
print(f"  beta1 (slope)     = {beta1:.6f}  (~{beta1*100:.2f}%)")
print(f"  beta2 (curvature) = {beta2:.6f}  (~{beta2*100:.2f}%)")
print(f"  lambda (years)    = {lam:.6f}")
print(f"Fit: RMSE={rmse_bp:.2f} bp, MAE={mae_bp:.2f} bp")

grid = np.linspace(0.25, 30.0, 240)
curve = ns_curve(grid, beta0, beta1, beta2, lam) * 100
plt.figure(figsize=(7, 4.5))
plt.scatter(m, y*100, label="Observed")
plt.plot(grid, curve, label="Nelson–Siegel fit")
plt.xlabel("Maturity (years)")
plt.ylabel("Yield (%)")
plt.title("South Africa Yield Curve — Nelson–Siegel Fit")
plt.legend()
plt.tight_layout()
plt.show()

"""### 2d. Fit a Cubic-Spline model"""

# Use your existing data (manual or CSV). Example manual entry:
df = pd.DataFrame({
    "Maturity": ["3M (T-bill)", "5Y", "10Y", "20Y", "30Y"],
    "Tenor_Years": [0.25, 5, 10, 20, 30],
    "Yield_%": [6.55, 7.89, 9.17, 10.32, 10.16],
})
x = df["Tenor_Years"].to_numpy(float)
y = (df["Yield_%"]/100.0).to_numpy(float)  # decimal yields
n = len(x)

# Natural cubic spline coefficients via Thomas algorithm (pure NumPy)
h = np.diff(x)                 # interval lengths
alpha = np.zeros(n)
alpha[1:-1] = (3/h[1:])*(y[2:]-y[1:-1]) - (3/h[:-1])*(y[1:-1]-y[:-2])

# Tridiagonal system A*c = alpha, natural BC ⇒ c0=c_{n-1}=0
l = np.ones(n); mu = np.zeros(n); z = np.zeros(n)
l[0] = 1.0; mu[0] = 0.0; z[0] = 0.0
for i in range(1, n-1):
    l[i] = 2*(x[i+1]-x[i-1]) - h[i-1]*mu[i-1]
    mu[i] = h[i]/l[i]
    z[i] = (alpha[i] - h[i-1]*z[i-1]) / l[i]
l[-1] = 1.0; z[-1] = 0.0

c = np.zeros(n); b = np.zeros(n-1); d = np.zeros(n-1)
# back substitution
for j in range(n-2, -1, -1):
    c[j] = z[j] - mu[j]*c[j+1]
    b[j] = ((y[j+1]-y[j])/h[j]) - h[j]*(2*c[j] + c[j+1])/3
    d[j] = (c[j+1]-c[j])/(3*h[j])

# Spline evaluation
def spline_eval(xq):
    xq = np.atleast_1d(xq)
    i = np.clip(np.searchsorted(x, xq) - 1, 0, n-2)  # interval index
    dx = xq - x[i]
    return y[i] + b[i]*dx + c[i]*dx**2 + d[i]*dx**3

# In-sample fit (for interpolating natural spline)
yhat_obs = spline_eval(x)
residuals_bp = (y - yhat_obs) * 1e4  # should be ~0
rmse_bp = np.sqrt(np.mean((residuals_bp)**2))

print(f"(d) Cubic-spline (natural, interpolating): RMSE ≈ {rmse_bp:.2f} bp (in-sample)")
print("Observed vs Fitted (bp residuals):")
print(np.round(residuals_bp, 4))

# Dense curve and plot
grid = np.linspace(x.min(), max(30.0, x.max()), 240)
curve = spline_eval(grid) * 100  # percent

plt.figure(figsize=(7, 4.5))
plt.scatter(x, y*100, label="Observed")
plt.plot(grid, curve, label="Natural cubic spline")
plt.xlabel("Maturity (years)")
plt.ylabel("Yield (%)")
plt.title("South Africa Yield Curve — Cubic Spline (Natural, no SciPy)")
plt.legend()
plt.tight_layout()
plt.show()

"""### 3a. Generate 5 uncorrelated Gaussian random variables"""

# Set Random Seed
np.random.seed(42)

# Generate 5 Gaussian variables
days_n = 126  # roughly 6 months of trading days((6*30)-(9*6)) #assuming there are 9 non-trading days per month
series_n = 5
means = np.zeros(series_n)

# Set the small std deviations
stds = np.array([0.02, 0.018, 0.015, 0.017, 0.016])

# create independent series
data = np.vstack([np.random.normal(loc=means[i], scale=stds[i], size=days_n) for i in range(series_n)]).T

#compute uncorrelated Guassian values
dates = pd.date_range(end=pd.Timestamp("2025-10-06"), periods=days_n, freq='B')  # business days end (at Oct 6 2025)
df_uncorr = pd.DataFrame(data, index=dates, columns=[f"Series_{i+1}" for i in range(series_n)])

"""### 3b. Compute PCA using the covariance matrix"""

# Balance the data
X = df_uncorr.values - df_uncorr.values.mean(axis=0)
cov = np.cov(X, rowvar=False)
eigvals, eigvecs = np.linalg.eigh(cov)  # eigh returns ascending eigenvalues

# sort result in descending order
order = np.argsort(eigvals)[::-1]
eigvals = eigvals[order]
eigvecs = eigvecs[:, order]

explained_variance = eigvals
explained_variance_ratio = explained_variance / explained_variance.sum()

"""### 3c. Print explained variances and percentages for components"""

#Set the container for components
explained_table = pd.DataFrame({
    "Component": [f"PC{i+1}" for i in range(series_n)],
    "Eigenvalue (Variance)": np.round(explained_variance, 8),
    "Explained Variance Ratio": np.round(explained_variance_ratio, 6)
})

print("PCA - Uncorrelated Gaussian Series (Covariance PCA)", explained_table)

"""### 3d. Produce a screeplot of variance explained by each component"""

plt.figure(figsize=(6,4))
plt.plot(np.arange(1, series_n+1), explained_variance_ratio, marker='o')
plt.title("Screeplot — Explained Variance Ratio (Uncorrelated Gaussian Series)")
plt.xlabel("Principal Component")
plt.ylabel("Explained Variance Ratio")
plt.xticks(np.arange(1, series_n+1))
plt.grid(True)
plt.show()

# Print the numeric breakdown for inclusion in the assistant message
explained_table.to_csv('pca_explained_variance.csv', index=False)
print("Saved numeric explained variance table")
print(explained_table.to_string(index=False))

"""### 3e. Collect the daily closing yields for 5 government securities"""

# Define the 5 treasury maturities
series_ids = ["DGS1MO","DGS3MO","DGS2","DGS5","DGS10"]
end_date = pd.to_datetime("2025-10-06") #I used today's date
start_date = end_date - pd.DateOffset(months=6)  #6 months as requested
start_str = start_date.strftime("%Y-%m-%d")
end_str   = end_date.strftime("%Y-%m-%d")
use_correlation = False  # False -> covariance PCA; True -> correlation PCA (standardize first)

def fetch_fred_series_csv(series_id, start_date, end_date):
    # FRED CSV graph endpoint
    url = f"https://fred.stlouisfed.org/graph/fredgraph.csv?id={series_id}&start_date={start_date}&end_date={end_date}"
    r = requests.get(url)
    r.raise_for_status()

    # Read the CSV without parsing dates initially to inspect columns
    df = pd.read_csv(StringIO(r.text))
    print(f"Columns from FRED for {series_id}: {df.columns.tolist()}") # Print columns

    # Assuming the first column is the date column based on FRED CSV structure
    date_col_name = df.columns[0]
    df = pd.read_csv(StringIO(r.text), parse_dates=[date_col_name])
    df.columns = ["DATE", series_id] # Rename the date column to 'DATE' and the value column
    return df.set_index("DATE")

# Fetch all series and align on business days
dfs = []
for s in series_ids:
    df = fetch_fred_series_csv(s, start_str, end_str)
    dfs.append(df)

df_all = pd.concat(dfs, axis=1)

# Drop rows with missing yields (e.g., weekends or missing obs)
df_all = df_all.dropna(how='all').sort_index()

display(df_all.head(8)) # Display head of the combined dataframe

# Heatmap of missing data
plt.figure(figsize=(10,6))
sns.heatmap(df_all.isnull(), cbar=False, cmap='viridis')
plt.title('Missing Data Heatmap', fontsize=14)
plt.show()

# Matrix visualization (from missingno)
msno.matrix(df_all)
plt.show()

# Bar plot showing missing values count
msno.bar(df_all)
plt.show()

# Convert missing values to binary indicators
missing_df = df_all.isnull().astype(int)

# Correlation heatmap of missingness
plt.figure(figsize=(10,6))
sns.heatmap(missing_df.corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Between Missingness of Variables', fontsize=14)
plt.show()

#KNN imputation (for MAR)

imputer_knn = KNNImputer(n_neighbors=5)
df_knn_imputed = imputer_knn.fit_transform(df.select_dtypes(include=['float64', 'int64']))
display(df_knn_imputed)

"""### 3f. compute daily yield changes"""

# Define first differences
df_changes = df_all.diff().dropna(how='all')

# Optional: if series contains occasional '.' or '.' strings for missing values, convert to numeric
df_changes = df_changes.apply(pd.to_numeric, errors='coerce').dropna(axis=0, how='any')  # keep days where all series available

"""### 3g. Perform PCA"""

X = df_changes.values
if use_correlation:
    # standardize
    Xz = (X - X.mean(axis=0)) / X.std(axis=0, ddof=1)
    pca = PCA()
    pca.fit(Xz)
else:
    # covariance PCA: center but don't scale
    Xc = X - X.mean(axis=0)
    pca = PCA()
    pca.fit(Xc)

explained_variance = pca.explained_variance_
explained_variance_ratio = pca.explained_variance_ratio_

"""### 3h. Print explained variance breakdown"""

#Set components of explained variance
ev_table = pd.DataFrame({
    "Component": [f"PC{i+1}" for i in range(len(explained_variance))],
    "Eigenvalue (variance)": explained_variance,
    "Explained variance ratio": explained_variance_ratio
})
print(ev_table)

"""### 3i. Screeplot for each variance component"""

#Plot the graph for explained variance ratio and principal components
plt.figure(figsize=(6,4))
plt.plot(np.arange(1, len(explained_variance_ratio)+1), explained_variance_ratio, marker='o')
plt.title("Screeplot — Explained Variance Ratio (Treasury daily yield changes)")
plt.xlabel("Principal Component")
plt.ylabel("Explained Variance Ratio")
plt.xticks(np.arange(1, len(explained_variance_ratio)+1))
plt.grid(True)
plt.show()



"""### 4a. Find the 30 largest holdings."""

# Highlight the preferred tickers (top 30)

tickers = [
    "COST","PG","WMT","KO","PM","PEP","MDLZ","MO","CL","TGT",
    "KMB","KVUE","KR","MNST","KDP","SYY","GIS","STZ","CHD","KHC",
    "HSY","ADM","K","MKC","DG","TSN","CLX","EL","DLTR","CAG"
]

"""### 4b. Get at least 6 months of data (~ 120 data points)"""

#Choose date range: last ~6 months (approx 120 trading days)
end = pd.Timestamp("today").date()
start = end - timedelta(days=185)   # ~6 months calendar days (factoring ~120 trading days)

# Download Adjusted Close
prices = yf.download(tickers, start=start.isoformat(), end=end.isoformat(), progress=False)

# Print column names to understand the structure
print("Downloaded data columns:", prices.columns)

# Access 'Close' level in the MultiIndex
if isinstance(prices.columns, pd.MultiIndex):
    try:
        prices = prices['Close']
    except KeyError:
        print("Warning: 'Close' level not found. Checking for direct 'Close' column.")
        if 'Close' in prices.columns:
             prices = prices['Close']
        else:
             raise KeyError("Could not find 'Close' as a column or a level in the downloaded data.")
elif 'Close' in prices.columns:
    prices = prices['Close']
else:
    raise KeyError("Could not find 'Close' as a column or a level in the downloaded data.")


# If yfinance returns a DataFrame with columns only for available tickers, check for NaNs.

# Drop columns with too many missing values or forward/backfill small gaps
prices = prices.dropna(axis=1, how='all')  # drop tickers with no data
prices = prices.fillna(method='ffill').fillna(method='bfill')  # simple gap fill for small gaps

# Keep only the top-30 tickers present
print("Downloaded price data shape:", prices.shape)
display(prices.tail())

"""### 4c. Compute the daily returns."""

# Compute log daily returns
log_returns = np.log(prices / prices.shift(1)).dropna(how='all')
print("Log returns shape:", log_returns.shape)
display(log_returns.head())

# Plot sample of price series and returns
plt.figure(figsize=(10,4))
plt.title("XLP Top-30: Adjusted Close (last ~6 months)")
prices.plot(legend=False)
plt.tight_layout()
plt.show()

plt.figure(figsize=(10,4))
plt.title("Sample of daily log returns (first 10 assets)")
log_returns.iloc[:, :10].plot(legend=False)
plt.tight_layout()
plt.show()

"""### 4d. Compute the covariance matrix"""

# Covariance matrix (sample covariance of returns)
cov = log_returns.cov()
print("Covariance matrix shape:", cov.shape)
display(cov.iloc[:8,:8])  # show top-left block

# Plot covariance heatmap
plt.figure(figsize=(12,10)) # Increased figure size
plt.title("Covariance matrix (returns)", fontsize=14) # Increased title font size
plt.imshow(cov, aspect='auto', interpolation='nearest')
plt.colorbar(label='Covariance')
plt.xticks(ticks=np.arange(len(cov.columns)), labels=cov.columns, rotation=90, fontsize=10) # Increased tick font size and rotated labels
plt.yticks(ticks=np.arange(len(cov.index)), labels=cov.index, fontsize=10) # Increased tick font size
plt.tight_layout()
plt.show()

"""### 4e. Compute the PCA

"""

# Since covariance is symmetric, eigh is used
eigvals, eigvecs = eigh(cov.values)   # eigvals ascending

# reverse to descending
eigvals = eigvals[::-1]
eigvecs = eigvecs[:, ::-1]

# Variance explained
total_var = eigvals.sum()
explained_variance_ratio = eigvals / total_var

print("Top 10 eigenvalues:", eigvals[:10])
print("Explained variance ratio (top 10):", explained_variance_ratio[:10])

# Scree plot
plt.figure(figsize=(10,6)) # Increased figure size
plt.plot(np.arange(1, len(eigvals)+1), eigvals, marker='o')
plt.title("Scree plot: Eigenvalues of covariance matrix", fontsize=14) # Increased title font size
plt.xlabel("Principal component (descending)", fontsize=12) # Increased label font size
plt.ylabel("Eigenvalue (variance explained)", fontsize=12) # Increased label font size
plt.xticks(fontsize=10) # Increased tick font size
plt.yticks(fontsize=10) # Increased tick font size
plt.grid(True)
plt.tight_layout()
plt.show()

# Show loadings of first 3 principal components
pc_df = pd.DataFrame(eigvecs[:, :3], index=cov.columns, columns=['PC1','PC2','PC3'])
print("Top loadings for PC1:")
display(pc_df['PC1'].sort_values(ascending=False).head(10))
display(pc_df['PC1'].sort_values().head(10))  # bottom

# Bar plot of PC1 loadings
plt.figure(figsize=(12,6)) # Increased figure size
plt.title("PC1 loadings (asset contributions)", fontsize=14) # Increased title font size
plt.bar(pc_df.index, pc_df['PC1'])
plt.xticks(rotation=90, fontsize=10) # Increased tick font size and rotated labels
plt.yticks(fontsize=10) # Increased tick font size
plt.tight_layout()
plt.show()

"""### 4f. Compute the SVD."""

# SVD on centered returns matrix
X = log_returns.values  # shape (T, N)
X_centered = X - X.mean(axis=0)

# compute SVD
U, S, Vt = svd(X_centered, full_matrices=False)  # S is 1D array of singular values
# SVD relationship: singular values S_i, Vt rows are right singular vectors
print("Singular values (first 10):", S[:10])

# Compare singular values^2 / (T-1) with eigenvalues of covariance:
# Note: cov = (1/(T-1)) * X_centered^T X_centered; thus eigenvalues ≈ (S^2)/(T-1)
T = X_centered.shape[0]
sv_to_eig = (S**2)/(T-1)

# Compare top entries
for i in range(min(6, len(eigvals))):
    print(f"PC{i+1}: eigval={eigvals[i]:.6f}, S^2/(T-1)={sv_to_eig[i]:.6f}")

# Plot singular values
plt.figure(figsize=(10,6)) # Increased figure size
plt.plot(np.arange(1, len(S)+1), S, marker='o')
plt.title("Singular values (SVD) of centered returns matrix", fontsize=14) # Increased title font size
plt.xlabel("Component index", fontsize=12) # Increased label font size
plt.ylabel("Singular value", fontsize=12) # Increased label font size
plt.xticks(fontsize=10) # Increased tick font size
plt.yticks(fontsize=10) # Increased tick font size
plt.grid(True)
plt.tight_layout()
plt.show()

# Project time-series onto top PCs (scores)
# PC scores (time series of principal components) can be computed as:
scores = X_centered.dot(eigvecs[:, :3])   # (T x 3)
scores_df = pd.DataFrame(scores, index=log_returns.index, columns=['PC1','PC2','PC3'])
display(scores_df.head())

#Plot the graph
plt.figure(figsize=(10,6)) # Increased figure size
plt.title("PC1 time series (component scores)", fontsize=14) # Increased title font size
scores_df['PC1'].plot()
plt.xticks(fontsize=10) # Increased tick font size
plt.yticks(fontsize=10) # Increased tick font size
plt.tight_layout()
plt.show()

# Save the output
cov.to_csv("xlp_top30_covariance.csv")
pc_df.to_csv("xlp_pc_loadings_top3.csv")
sv_df = pd.DataFrame(Vt.T[:, :3], index=cov.columns, columns=['SV1','SV2','SV3']) # Corrected to use Vt.T
sv_df.to_csv("xlp_svd_rightsingular_top3.csv")
print("Saved covariance and CSVs to current directory.")

# Run simple diagnostics
print("Total variance (sum of eigvals):", total_var)
print("Cumulative explained variance (top 5 PCs):", explained_variance_ratio[:5].cumsum())



"""**Empirical PCA and SVD Analysis of XLP (Consumer Staples)**

We chose the Consumer Staples Sector SPDR (XLP) as our sample ETF because its largest constituents are major consumer firms like Walmart (WMT), Costco (COST), Procter & Gamble (PG), Coca-Cola (KO), and Philip Morris (PM) (“XLP Holdings List - Consumer Staples Select Sector SPDR Fund”). We acquired ~6 months of daily prices for XLP's 30 largest constituents and calculated log-daily returns (each day's percentage change). We used returns instead of raw prices because it is scale-invariant (all of an asset's returns are in analogous percentage units and are likely to be stationary, whereas prices have unit roots)(“Why Do We Usually Model Returns and Not Prices?”, “Why Log Returns”). Practically, using log-returns makes returns accumulated over multiple periods of time (e.g., weeks, quarters), additive and more normally-distributed for risk modeling purposes (“Why Log Returns”). Normalization also gives a meaningful interpretation to the covariance matrix by revealing how each pair of assets' percentage returns co-move.

Having gotten our returns matrix, we then calculated the sample covariance matrix of all 30 assets' returns. This covariance matrix is key to portfolio analysis because each off-diagonal element reveals a pair of stocks' co-movement (positive covariance implies they tend to both rise and fall, negative implies opposite motion). The diagonal elements are each asset's returns variance (each asset's volatility). Therefore, the covariance matrix summarizes the portfolio's risk and correlation structure(“Why Do We Usually Model Returns and Not Prices?”). Stocks, for instance, which are in analogous sub-industries, tend to have higher covariation.

**Principal Component Analysis (PCA)**
We used PCA by eigendecomposing the covariance matrix. PCA finds new orthogonal directions (principal components) which capture maximal variance of the data (“Principal Component Analysis”, “Principal Component Analysis (PCA): Explained Step-by-Step”). Mathematically, the principal components are the eigenvectors of the covariance matrix. The first eigenvector (principal component) is the direction in 30-dimensional return-space explaining the largest total variance; the second eigenvector is orthogonal to the first and explains the next largest variance, etc (“Principal Component Analysis”, “Principal Component Analysis (PCA): Explained Step-by-Step”). That is, each eigenvector describes a linear combination (factor) of the 30 stocks, and its related eigenvalue describes how much variance that factor explains. Large eigenvalues correspond to strong factors. Intuitively, in financial terms, we find here that first few principal components frequently correspond to broad market/sector factors, and later components correspond to smaller variation sources (idiosyncratic moves). For instance, it's a known empirical fact that first principal component of stock returns tends to have all coefficients of same sign, essentially corresponding to the "market factor"/equity risk premium. Indeed, our calculated first principal component has nearly constant positive loadings on nearly all constituents of XLP, indicating a common market-level effect (e.g. overall consumer spending patterns) (“Why Is the First Principal Component a Proxy for the Market Portfolio, and What Other Proxies Exist?”). Successive components have mixed signs and capture more industry-specific patterns.

The eigenvalues of the covariance matrix actually quantify how many of variance each principal component explains (“Principal Component Analysis (PCA): Explained Step-by-Step”). With our data, largest eigenvalue may explain, say, 30–40% total variance, and successive eigenvalue explains progressively less. This is often illustrated by a scree plot. Intuitively, for portfolio risk, largest eigenvectors (with largest eigenvalues) correspond to dominant risk factors. In other words, a very large first eigenvalue suggests a lot of co-movement being caused by a single common factor. We saw our first 3–4 eigenvalues of our XLP data absorb a lot of variance, indicating a few latent factors are enough to characterize most of the variability of returns. The eigenvectors themselves (the "loadings" or weights for each stock) explain how each stock participates in each factor. For example, if we find the first eigenvector to have about equal positive weights, it means all stocks are moving together by means of that principal direction (“Why Is the First Principal Component a Proxy for the Market Portfolio, and What Other Proxies Exist?”, “Principal Component Analysis (PCA): Explained Step-by-Step”). Low-ranked eigenvectors (small eigenvalues) are related to niche patterns (e.g. one might capture the behavior of a specific stock or sub-sector) and don't contribute to overall variance much (“Principal Component Analysis (PCA): Explained Step-by-Step”).

**Singular Value Decomposition (SVD)**
We also performed Singular Value Decomposition on the centered returns matrix X (rows=time, columns=assets). SVD decomposes X into X = U Σ Vᵀ (Wikipedia). Here U (T×T) holds left singular vectors (an orthonormal basis for time-domain patterns), V (30×30) holds right singular vectors (asset loadings), and Σ is a diagonal matrix of non-negative singular values(Wikipedia, Wikipedia). Every singular value σᵢ (Σ entry) is equivalent to the square root of the ith eigenvalue of the covariance matrix (“Task 4 | Principal Component Analysis | Eigenvalues and Eigenvectors”, “Singular Value Decomposition”). Indeed, one discovers that applying PCA to the covariance matrix or SVD to data produces the identical set of orthogonal components for centered data. In SVD, the first right singular vector coincides with the first principal component of PCA, and its singular value σ₁ is the square root of the first eigenvalue (so σ₁² = λ₁, variance explained). The benefit of SVD is directly working on (potentially rectangular) data matrix and being numerically stable ("PCA, Eigen Decomposition and SVD", “Singular Value Decomposition”). Here, in our context, the right singular vectors in V are merely principal component directions across assets (identical to PCA eigenvectors), and left singular vectors in U encode each component's time-series "scores." Diagonal singular values in Σ express how significantly each component impacts data's variance (“Singular Value Decomposition”). Large σ₁, for example, means first component (market factor) dominates, whereas tiny σₙ signifies nth component adds negligibly. We saw, practically, singular values plummeted swiftly, verifying that a handful of components retain majority of structure.

**Comparisons between PCA and SVD and Interpretation**
Both PCA and SVD aim to reduce dimensionality by finding the most informative directions. In PCA, one explicitly computes the covariance matrix and performs eigen-decomposition. SVD bypasses that by directly decomposing the data matrix. PCA has a clear statistical interpretation (variance explained by each component) whereas SVD is more a general algebraic factorization ("PCA, Eigen Decomposition and SVD", “Singular Value Decomposition”). In practice, when data is mean-centered, the two methods yield equivalent information. Conceptually, PCA is tied to the covariance structure of returns, while SVD is a more general tool that can be applied even when data is not square  ("PCA, Eigen Decomposition and SVD", “Singular Value Decomposition”).

For our ETF data, we interpret the results as follows: the eigenvectors (or right-singular vectors) show the principal patterns of co-movement among the 30 stocks. The first eigenvector has all-positive weights, confirming it captures the overall consumer-sector movement (“Why Is the First Principal Component a Proxy for the Market Portfolio, and What Other Proxies Exist?”). Each subsequent eigenvector has weights that highlight contrasts (for example, one eigenvector might load positively on consumer staples versus negatively on tobacco, isolating different industry swings). The eigenvalues (and squared singular values) tell us how much variance each pattern explains (“Principal Component Analysis (PCA): Explained Step-by-Step”, “Singular Value Decomposition”). In our case, the first eigenvalue is largest (strong market factor), the next few are moderately large (sub-sector factors), and the rest are small. The singular values (√eigenvalues) likewise quantify these strengths. Thus, PCA/SVD reveal that the 30-dimensional return data is largely driven by a few latent factors: one broad market factor and a handful of sector-specific factors. This insight is useful for portfolio risk management, as it shows that diversifying across all 30 stocks primarily addresses one or two main sources of risk rather than 30 independent ones.

In summary, computing daily returns and analyzing their covariance via PCA/SVD uncovers the underlying factor structure. Returns are the natural input because they standardize different-priced assets and make variances interpretable (quant.stachex, “Why Log Returns”). PCA identifies uncorrelated principal portfolios (eigenvectors) and quantifies their importance (eigenvalues) (“Principal Component Analysis (PCA): Explained Step-by-Step”, “Singular Value Decomposition”). SVD provides an equivalent decomposition on the raw returns matrix. Together, these techniques help us understand and reduce the dimensionality of the return data, highlighting that most variance comes from a few key factors in the consumer staples sector (“Why Is the First Principal Component a Proxy for the Market Portfolio, and What Other Proxies Exist?”, “Principal Component Analysis (PCA): Explained Step-by-Step”).
"""



"""**References**

PCA, Eigen Decomposition and SVD. Michigan Technological University, https://pages.mtu.edu/~shanem/psy5220/daily/Day04/PCA.html

Principal Component Analysis. Wikipedia: The Free Encyclopedia, Wikimedia Foundation, https://en.wikipedia.org/wiki/Principal_component_analysis

Principal Component Analysis (PCA): Explained Step-by-Step. Built In, https://builtin.com/data-science/step-step-explanation-principal-component-analysis

Singular Value Decomposition. Wikipedia: The Free Encyclopedia, Wikimedia Foundation, https://en.wikipedia.org/wiki/Singular_value_decomposition

Task 4 | Principal Component Analysis | Eigenvalues and Eigenvectors. Scribd, https://www.scribd.com/document/924321982/Task-4

Why Do We Usually Model Returns and Not Prices?” Quantitative Finance Stack Exchange, https://quant.stackexchange.com/questions/16481/why-do-we-usually-model-returns-and-not-prices

Why Is the First Principal Component a Proxy for the Market Portfolio, and What Other Proxies Exist? Quantitative Finance Stack Exchange, https://quant.stackexchange.com/questions/2679/why-is-the-first-principal-component-a-proxy-for-the-market-portfolio-and-what

Why Log Returns. Quantivity, 21 Feb. 2011, https://quantivity.wordpress.com/2011/02/21/why-log-returns/

XLP ETF Sector Allocation. MarketXLS, MarketXLS Limited, https://marketxls.com/etfs/xlp/sectors
.
XLP Holdings List - Consumer Staples Select Sector SPDR Fund. Stock Analysis, https://stockanalysis.com/etf/xlp/holdings/

Top 30 holdings (XLP) — tickers + weights (as of early Oct 2025)

COST — 9.71%

PG — 9.24%

WMT — 8.81%

KO — 6.42%

PM — 5.71%

PEP — 4.76%

MDLZ — 4.55%

MO — 4.53%

CL — 4.50%

TGT — 2.90%

KMB — 2.86%

KVUE — 2.79%

KR — 2.73%

MNST — 2.48%

KDP — 2.48%

SYY — 2.23%

GIS — 1.998%

STZ (Constellation) — 1.73%

CHD — 1.64%

KHC — 1.61%

HSY — 1.53%

ADM — 1.39%

K — 1.34%

MCX (McCormick) — 1.26%

DG — 1.17%

TSN — 1.11%

CLX — 1.10%

EL — 0.94%

DLTR — 0.92%

CAG — 0.77%

(Source: MarketXLS / official fund pages; holdings as at weeek 1, Oct 2025)
"""